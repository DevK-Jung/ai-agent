{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Generator Node Test Notebook\n",
    "\n",
    "This notebook tests the `generator.py` module functionality with various question types and contexts."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T06:56:40.199731Z",
     "start_time": "2026-02-20T06:56:40.190511Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# .env 파일 로드\n",
    "dotenv_path = find_dotenv()\n",
    "load_dotenv(dotenv_path, override=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T06:56:44.561329Z",
     "start_time": "2026-02-20T06:56:41.988351Z"
    }
   },
   "source": [
    "# Import required modules\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from app.agents.nodes.generator import generate_answer\n",
    "from app.agents.prompts.generation import PROMPT_MAP, ERROR_MESSAGE\n",
    "from app.agents.state import ChatState\n",
    "import time"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimjunghyeon/Desktop/workspace/ai-agent/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Setup\n",
    "\n",
    "Create mock chat states with different question types and contexts for testing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T06:57:02.505562Z",
     "start_time": "2026-02-20T06:57:02.502099Z"
    }
   },
   "source": [
    "# Sample context for testing\n",
    "SAMPLE_CONTEXTS = {\n",
    "    \"python_info\": \"\"\"\n",
    "Python은 1991년 귀도 반 로섬(Guido van Rossum)이 개발한 고급 프로그래밍 언어입니다.\n",
    "Python의 주요 특징:\n",
    "- 간단하고 읽기 쉬운 문법\n",
    "- 인터프리터 언어\n",
    "- 객체지향 프로그래밍 지원\n",
    "- 풍부한 라이브러리 생태계\n",
    "- 크로스 플랫폼 지원\n",
    "\n",
    "Python은 웹 개발, 데이터 분석, 머신러닝, 자동화 등 다양한 분야에서 사용됩니다.\n",
    "\"\"\",\n",
    "    \n",
    "    \"framework_comparison\": \"\"\"\n",
    "Django vs FastAPI 비교:\n",
    "\n",
    "Django:\n",
    "- 2005년 출시\n",
    "- Full-stack 웹 프레임워크\n",
    "- ORM, 관리자 패널, 인증 시스템 내장\n",
    "- MTV(Model-Template-View) 패턴\n",
    "- 대규모 웹 애플리케이션에 적합\n",
    "\n",
    "FastAPI:\n",
    "- 2018년 출시\n",
    "- 고성능 API 프레임워크\n",
    "- 자동 API 문서화 (Swagger/OpenAPI)\n",
    "- Type hints 기반 검증\n",
    "- 비동기 처리 최적화\n",
    "- REST API 개발에 특화\n",
    "\"\"\",\n",
    "    \n",
    "    \"machine_learning\": \"\"\"\n",
    "머신러닝(Machine Learning)은 인공지능의 한 분야로, 컴퓨터가 명시적으로 프로그래밍되지 않고도\n",
    "데이터로부터 학습하여 패턴을 찾고 예측을 수행하는 기술입니다.\n",
    "\n",
    "주요 유형:\n",
    "1. 지도학습 (Supervised Learning): 레이블된 데이터로 학습\n",
    "   - 예시: 이미지 분류, 스팸 메일 필터링\n",
    "2. 비지도학습 (Unsupervised Learning): 레이블 없는 데이터로 패턴 발견\n",
    "   - 예시: 클러스터링, 이상 탐지\n",
    "3. 강화학습 (Reinforcement Learning): 환경과의 상호작용을 통한 학습\n",
    "   - 예시: 게임 AI, 로봇 제어\n",
    "\n",
    "Python의 대표적인 머신러닝 라이브러리:\n",
    "- scikit-learn: 전통적 머신러닝 알고리즘\n",
    "- TensorFlow: 구글에서 개발한 딥러닝 프레임워크\n",
    "- PyTorch: 페이스북에서 개발한 딥러닝 프레임워크\n",
    "- pandas: 데이터 조작 및 분석\n",
    "- numpy: 수치 계산\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "def create_test_state(question_type, messages, context=\"\", user_id=\"test_user\", session_id=\"test_session\"):\n",
    "    \"\"\"테스트용 ChatState 생성\"\"\"\n",
    "    return {\n",
    "        \"messages\": messages,\n",
    "        \"question_type\": question_type,\n",
    "        \"context\": context,\n",
    "        \"user_id\": user_id,\n",
    "        \"session_id\": session_id,\n",
    "        \"thread_id\": f\"thread_{session_id}\",\n",
    "        \"model_used\": \"\"\n",
    "    }\n",
    "\n",
    "print(\"Test setup completed!\")\n",
    "print(f\"Available prompt types: {list(PROMPT_MAP.keys())}\")\n",
    "print(f\"Error message: {ERROR_MESSAGE}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test setup completed!\n",
      "Available prompt types: ['FACT', 'SUMMARY', 'COMPARE', 'EVIDENCE']\n",
      "Error message: 답변 생성 중 오류가 발생했습니다. 다시 시도해주세요.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case 1: FACT Questions\n",
    "\n",
    "Test generation of factual answers with context"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T06:57:12.777996Z",
     "start_time": "2026-02-20T06:57:06.868503Z"
    }
   },
   "source": [
    "async def test_fact_generation():\n",
    "    print(\"=== FACT Question Generation Test ===\")\n",
    "    \n",
    "    test_cases = [\n",
    "        {\n",
    "            \"description\": \"Basic Python information\",\n",
    "            \"messages\": [HumanMessage(content=\"Python이란 무엇인가요?\", id=\"msg1\")],\n",
    "            \"context\": SAMPLE_CONTEXTS[\"python_info\"]\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"Python features inquiry\",\n",
    "            \"messages\": [\n",
    "                AIMessage(content=\"안녕하세요!\", id=\"msg1\"),\n",
    "                HumanMessage(content=\"Python의 주요 특징은 무엇인가요?\", id=\"msg2\")\n",
    "            ],\n",
    "            \"context\": SAMPLE_CONTEXTS[\"python_info\"]\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"Question without context\",\n",
    "            \"messages\": [HumanMessage(content=\"JavaScript란 무엇인가요?\", id=\"msg1\")],\n",
    "            \"context\": \"\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        print(f\"\\nTest {i}: {test_case['description']}\")\n",
    "        \n",
    "        # Create test state\n",
    "        state = create_test_state(\n",
    "            question_type=\"FACT\",\n",
    "            messages=test_case[\"messages\"],\n",
    "            context=test_case[\"context\"]\n",
    "        )\n",
    "        \n",
    "        # Show input info\n",
    "        last_human_msg = next(\n",
    "            (msg.content for msg in reversed(test_case[\"messages\"]) if isinstance(msg, HumanMessage)),\n",
    "            \"No question found\"\n",
    "        )\n",
    "        print(f\"  Question: {last_human_msg}\")\n",
    "        print(f\"  Context available: {'Yes' if test_case['context'] else 'No'}\")\n",
    "        print(f\"  Context length: {len(test_case['context'])} chars\")\n",
    "        \n",
    "        # Generate answer\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            result = await generate_answer(state)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            answer = result.get(\"answer\", \"\")\n",
    "            model_used = result.get(\"model_used\", \"\")\n",
    "            messages = result.get(\"messages\", [])\n",
    "            \n",
    "            print(f\"  Model: {model_used}\")\n",
    "            print(f\"  Generation time: {end_time - start_time:.2f}s\")\n",
    "            print(f\"  Answer length: {len(answer)} chars\")\n",
    "            print(f\"  Answer preview: {answer[:200]}{'...' if len(answer) > 200 else ''}\")\n",
    "            print(f\"  Messages returned: {len(messages)}\")\n",
    "            \n",
    "            # Check if it's an error message\n",
    "            if answer == ERROR_MESSAGE:\n",
    "                print(f\"  ⚠️ Error response detected\")\n",
    "            else:\n",
    "                print(f\"  ✅ Answer generated successfully\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error: {e}\")\n",
    "\n",
    "await test_fact_generation()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FACT Question Generation Test ===\n",
      "\n",
      "Test 1: Basic Python information\n",
      "  Question: Python이란 무엇인가요?\n",
      "  Context available: Yes\n",
      "  Context length: 200 chars\n",
      "  Model: gpt-4o\n",
      "  Generation time: 1.41s\n",
      "  Answer length: 194 chars\n",
      "  Answer preview: Python은 1991년 귀도 반 로섬(Guido van Rossum)이 개발한 고급 프로그래밍 언어입니다. 주요 특징으로는 간단하고 읽기 쉬운 문법, 인터프리터 언어, 객체지향 프로그래밍 지원, 풍부한 라이브러리 생태계, 크로스 플랫폼 지원 등이 있습니다. Python은 웹 개발, 데이터 분석, 머신러닝, 자동화 등 다양한 분야에서 사용됩니다.\n",
      "  Messages returned: 1\n",
      "  ✅ Answer generated successfully\n",
      "\n",
      "Test 2: Python features inquiry\n",
      "  Question: Python의 주요 특징은 무엇인가요?\n",
      "  Context available: Yes\n",
      "  Context length: 200 chars\n",
      "  Model: gpt-4o\n",
      "  Generation time: 1.02s\n",
      "  Answer length: 141 chars\n",
      "  Answer preview: Python의 주요 특징은 다음과 같습니다:\n",
      "\n",
      "- 간단하고 읽기 쉬운 문법\n",
      "- 인터프리터 언어\n",
      "- 객체지향 프로그래밍 지원\n",
      "- 풍부한 라이브러리 생태계\n",
      "- 크로스 플랫폼 지원\n",
      "\n",
      "이러한 특징들 덕분에 Python은 다양한 분야에서 널리 사용되고 있습니다.\n",
      "  Messages returned: 1\n",
      "  ✅ Answer generated successfully\n",
      "\n",
      "Test 3: Question without context\n",
      "  Question: JavaScript란 무엇인가요?\n",
      "  Context available: No\n",
      "  Context length: 0 chars\n",
      "  Model: gpt-4o\n",
      "  Generation time: 3.48s\n",
      "  Answer length: 310 chars\n",
      "  Answer preview: JavaScript는 웹 브라우저에서 동작하는 대화형 웹 페이지를 만들기 위해 사용되는 프로그래밍 언어입니다. HTML과 CSS와 함께 웹의 3대 주요 기술 중 하나로, 클라이언트 측 스크립팅을 통해 사용자와의 상호작용을 가능하게 합니다. JavaScript는 웹 페이지의 콘텐츠를 변경하거나, 사용자 입력을 처리하고, 멀티미디어를 제어하는 등의 기능을 수행...\n",
      "  Messages returned: 1\n",
      "  ✅ Answer generated successfully\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case 2: SUMMARY Questions\n",
    "\n",
    "Test generation of summary responses"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T06:57:54.606765Z",
     "start_time": "2026-02-20T06:57:44.173546Z"
    }
   },
   "source": [
    "async def test_summary_generation():\n",
    "    print(\"=== SUMMARY Question Generation Test ===\")\n",
    "    \n",
    "    test_cases = [\n",
    "        {\n",
    "            \"description\": \"Summarize Python information\",\n",
    "            \"messages\": [HumanMessage(content=\"Python에 대해 요약해주세요\", id=\"msg1\")],\n",
    "            \"context\": SAMPLE_CONTEXTS[\"python_info\"]\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"Summarize machine learning\",\n",
    "            \"messages\": [HumanMessage(content=\"머신러닝에 대한 내용을 정리해주세요\", id=\"msg1\")],\n",
    "            \"context\": SAMPLE_CONTEXTS[\"machine_learning\"]\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"Summary without context\",\n",
    "            \"messages\": [HumanMessage(content=\"블록체인 기술을 요약해주세요\", id=\"msg1\")],\n",
    "            \"context\": \"\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        print(f\"\\nTest {i}: {test_case['description']}\")\n",
    "        \n",
    "        state = create_test_state(\n",
    "            question_type=\"SUMMARY\",\n",
    "            messages=test_case[\"messages\"],\n",
    "            context=test_case[\"context\"]\n",
    "        )\n",
    "        \n",
    "        last_human_msg = next(\n",
    "            (msg.content for msg in reversed(test_case[\"messages\"]) if isinstance(msg, HumanMessage)),\n",
    "            \"No question found\"\n",
    "        )\n",
    "        print(f\"  Question: {last_human_msg}\")\n",
    "        print(f\"  Context available: {'Yes' if test_case['context'] else 'No'}\")\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            result = await generate_answer(state)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            answer = result.get(\"answer\", \"\")\n",
    "            model_used = result.get(\"model_used\", \"\")\n",
    "            \n",
    "            print(f\"  Model: {model_used}\")\n",
    "            print(f\"  Generation time: {end_time - start_time:.2f}s\")\n",
    "            print(f\"  Answer length: {len(answer)} chars\")\n",
    "            print(f\"  Answer preview: {answer[:300]}{'...' if len(answer) > 300 else ''}\")\n",
    "            \n",
    "            if answer == ERROR_MESSAGE:\n",
    "                print(f\"  ⚠️ Error response detected\")\n",
    "            else:\n",
    "                print(f\"  ✅ Summary generated successfully\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error: {e}\")\n",
    "\n",
    "await test_summary_generation()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SUMMARY Question Generation Test ===\n",
      "\n",
      "Test 1: Summarize Python information\n",
      "  Question: Python에 대해 요약해주세요\n",
      "  Context available: Yes\n",
      "  Model: gpt-4o\n",
      "  Generation time: 2.39s\n",
      "  Answer length: 241 chars\n",
      "  Answer preview: Python은 1991년 귀도 반 로섬이 개발한 고급 프로그래밍 언어입니다. 주요 특징은 다음과 같습니다:\n",
      "\n",
      "- **문법**: 간단하고 읽기 쉬운 문법\n",
      "- **언어 유형**: 인터프리터 언어\n",
      "- **프로그래밍 패러다임**: 객체지향 프로그래밍 지원\n",
      "- **라이브러리**: 풍부한 라이브러리 생태계\n",
      "- **호환성**: 크로스 플랫폼 지원\n",
      "\n",
      "Python은 웹 개발, 데이터 분석, 머신러닝, 자동화 등 다양한 분야에서 널리 사용됩니다.\n",
      "  ✅ Summary generated successfully\n",
      "\n",
      "Test 2: Summarize machine learning\n",
      "  Question: 머신러닝에 대한 내용을 정리해주세요\n",
      "  Context available: Yes\n",
      "  Model: gpt-4o\n",
      "  Generation time: 2.46s\n",
      "  Answer length: 456 chars\n",
      "  Answer preview: 머신러닝은 인공지능의 한 분야로, 컴퓨터가 데이터로부터 학습하여 패턴을 찾고 예측을 수행하는 기술입니다.\n",
      "\n",
      "주요 유형:\n",
      "1. 지도학습 (Supervised Learning)\n",
      "   - 레이블된 데이터로 학습\n",
      "   - 예시: 이미지 분류, 스팸 메일 필터링\n",
      "\n",
      "2. 비지도학습 (Unsupervised Learning)\n",
      "   - 레이블 없는 데이터로 패턴 발견\n",
      "   - 예시: 클러스터링, 이상 탐지\n",
      "\n",
      "3. 강화학습 (Reinforcement Learning)\n",
      "   - 환경과의 상호작용을 통한 학습\n",
      "   - 예시: 게임 AI, 로봇 제어\n",
      "\n",
      "...\n",
      "  ✅ Summary generated successfully\n",
      "\n",
      "Test 3: Summary without context\n",
      "  Question: 블록체인 기술을 요약해주세요\n",
      "  Context available: No\n",
      "  Model: gpt-4o\n",
      "  Generation time: 5.58s\n",
      "  Answer length: 668 chars\n",
      "  Answer preview: 블록체인 기술 요약:\n",
      "\n",
      "1. 정의 및 기본 개념\n",
      "   - 블록체인은 데이터를 블록 단위로 저장하고 체인 형태로 연결한 분산 원장 기술이다.\n",
      "   - 탈중앙화: 중앙 기관 없이 네트워크 참여자들이 공동으로 데이터를 검증하고 기록한다.\n",
      "\n",
      "2. 주요 특징\n",
      "   - 투명성: 모든 거래 내역이 공개되어 누구나 조회할 수 있다.\n",
      "   - 보안성: 암호화 기술을 사용하여 데이터의 무결성과 보안을 유지한다.\n",
      "   - 변경 불가성: 한 번 기록된 데이터는 수정할 수 없어 신뢰성을 제공한다.\n",
      "\n",
      "3. 구성 요소\n",
      "   - 블록: 거래 데이터를 포함하며 ...\n",
      "  ✅ Summary generated successfully\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case 3: COMPARE Questions\n",
    "\n",
    "Test generation of comparison responses"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T06:58:28.964767Z",
     "start_time": "2026-02-20T06:58:00.546484Z"
    }
   },
   "source": [
    "async def test_compare_generation():\n",
    "    print(\"=== COMPARE Question Generation Test ===\")\n",
    "    \n",
    "    test_cases = [\n",
    "        {\n",
    "            \"description\": \"Django vs FastAPI comparison\",\n",
    "            \"messages\": [HumanMessage(content=\"Django와 FastAPI를 비교해주세요\", id=\"msg1\")],\n",
    "            \"context\": SAMPLE_CONTEXTS[\"framework_comparison\"]\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"Programming language comparison\",\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=\"안녕하세요\", id=\"msg1\"),\n",
    "                AIMessage(content=\"안녕하세요! 무엇을 도와드릴까요?\", id=\"msg2\"),\n",
    "                HumanMessage(content=\"Python과 Java의 차이점은 무엇인가요?\", id=\"msg3\")\n",
    "            ],\n",
    "            \"context\": SAMPLE_CONTEXTS[\"python_info\"]\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"Comparison without sufficient context\",\n",
    "            \"messages\": [HumanMessage(content=\"React와 Vue.js 중 어떤 것이 더 좋나요?\", id=\"msg1\")],\n",
    "            \"context\": \"\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        print(f\"\\nTest {i}: {test_case['description']}\")\n",
    "        \n",
    "        state = create_test_state(\n",
    "            question_type=\"COMPARE\",\n",
    "            messages=test_case[\"messages\"],\n",
    "            context=test_case[\"context\"]\n",
    "        )\n",
    "        \n",
    "        last_human_msg = next(\n",
    "            (msg.content for msg in reversed(test_case[\"messages\"]) if isinstance(msg, HumanMessage)),\n",
    "            \"No question found\"\n",
    "        )\n",
    "        print(f\"  Question: {last_human_msg}\")\n",
    "        print(f\"  Context available: {'Yes' if test_case['context'] else 'No'}\")\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            result = await generate_answer(state)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            answer = result.get(\"answer\", \"\")\n",
    "            model_used = result.get(\"model_used\", \"\")\n",
    "            \n",
    "            print(f\"  Model: {model_used}\")\n",
    "            print(f\"  Generation time: {end_time - start_time:.2f}s\")\n",
    "            print(f\"  Answer length: {len(answer)} chars\")\n",
    "            print(f\"  Answer preview: {answer[:300]}{'...' if len(answer) > 300 else ''}\")\n",
    "            \n",
    "            if answer == ERROR_MESSAGE:\n",
    "                print(f\"  ⚠️ Error response detected\")\n",
    "            else:\n",
    "                print(f\"  ✅ Comparison generated successfully\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error: {e}\")\n",
    "\n",
    "await test_compare_generation()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPARE Question Generation Test ===\n",
      "\n",
      "Test 1: Django vs FastAPI comparison\n",
      "  Question: Django와 FastAPI를 비교해주세요\n",
      "  Context available: Yes\n",
      "  Model: gpt-4o\n",
      "  Generation time: 8.76s\n",
      "  Answer length: 1713 chars\n",
      "  Answer preview: Django와 FastAPI는 Python으로 작성된 웹 프레임워크로, 각기 다른 장점과 특성을 가지고 있습니다. 아래 표를 통해 두 프레임워크의 공통점과 차이점을 비교하겠습니다:\n",
      "\n",
      "| 특징               | Django                                   | FastAPI                                       |\n",
      "|--------------------|------------------------------------------|---------------------...\n",
      "  ✅ Comparison generated successfully\n",
      "\n",
      "Test 2: Programming language comparison\n",
      "  Question: Python과 Java의 차이점은 무엇인가요?\n",
      "  Context available: Yes\n",
      "  Model: gpt-4o\n",
      "  Generation time: 6.83s\n",
      "  Answer length: 1765 chars\n",
      "  Answer preview: Python과 Java는 모두 널리 사용되는 프로그래밍 언어지만, 여러 면에서 차이가 있습니다. 아래 표를 통해 그 차이점과 공통점을 비교해 보겠습니다.\n",
      "\n",
      "| 특징                | Python                                                      | Java                                                        |\n",
      "|---------------------|--------------------------------------------...\n",
      "  ✅ Comparison generated successfully\n",
      "\n",
      "Test 3: Comparison without sufficient context\n",
      "  Question: React와 Vue.js 중 어떤 것이 더 좋나요?\n",
      "  Context available: No\n",
      "  Model: gpt-4o\n",
      "  Generation time: 12.83s\n",
      "  Answer length: 2357 chars\n",
      "  Answer preview: React와 Vue.js는 둘 다 인기 있는 JavaScript 프론트엔드 라이브러리로, 각각의 장단점이 있습니다. 두 라이브러리를 비교할 때 주요 요소들을 고려할 수 있습니다:\n",
      "\n",
      "| 요소            | React                                                                 | Vue.js                                                                |\n",
      "|-----------------|------------------...\n",
      "  ✅ Comparison generated successfully\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case 4: EVIDENCE Questions\n",
    "\n",
    "Test generation of evidence-based responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_evidence_generation():\n",
    "    print(\"=== EVIDENCE Question Generation Test ===\")\n",
    "    \n",
    "    test_cases = [\n",
    "        {\n",
    "            \"description\": \"Evidence for Python popularity\",\n",
    "            \"messages\": [HumanMessage(content=\"Python이 인기 있는 이유의 근거를 제시해주세요\", id=\"msg1\")],\n",
    "            \"context\": SAMPLE_CONTEXTS[\"python_info\"]\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"Evidence for machine learning effectiveness\",\n",
    "            \"messages\": [HumanMessage(content=\"머신러닝이 효과적인 이유를 증거와 함께 설명해주세요\", id=\"msg1\")],\n",
    "            \"context\": SAMPLE_CONTEXTS[\"machine_learning\"]\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"Evidence request without context\",\n",
    "            \"messages\": [HumanMessage(content=\"클라우드 컴퓨팅이 필요한 근거는 무엇인가요?\", id=\"msg1\")],\n",
    "            \"context\": \"\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        print(f\"\\nTest {i}: {test_case['description']}\")\n",
    "        \n",
    "        state = create_test_state(\n",
    "            question_type=\"EVIDENCE\",\n",
    "            messages=test_case[\"messages\"],\n",
    "            context=test_case[\"context\"]\n",
    "        )\n",
    "        \n",
    "        last_human_msg = next(\n",
    "            (msg.content for msg in reversed(test_case[\"messages\"]) if isinstance(msg, HumanMessage)),\n",
    "            \"No question found\"\n",
    "        )\n",
    "        print(f\"  Question: {last_human_msg}\")\n",
    "        print(f\"  Context available: {'Yes' if test_case['context'] else 'No'}\")\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            result = await generate_answer(state)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            answer = result.get(\"answer\", \"\")\n",
    "            model_used = result.get(\"model_used\", \"\")\n",
    "            \n",
    "            print(f\"  Model: {model_used}\")\n",
    "            print(f\"  Generation time: {end_time - start_time:.2f}s\")\n",
    "            print(f\"  Answer length: {len(answer)} chars\")\n",
    "            print(f\"  Answer preview: {answer[:300]}{'...' if len(answer) > 300 else ''}\")\n",
    "            \n",
    "            if answer == ERROR_MESSAGE:\n",
    "                print(f\"  ⚠️ Error response detected\")\n",
    "            else:\n",
    "                print(f\"  ✅ Evidence-based answer generated successfully\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error: {e}\")\n",
    "\n",
    "await test_evidence_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case 5: Edge Cases and Error Handling\n",
    "\n",
    "Test various edge cases and error scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_edge_cases():\n",
    "    print(\"=== Edge Cases and Error Handling Test ===\")\n",
    "    \n",
    "    test_cases = [\n",
    "        {\n",
    "            \"description\": \"Empty messages\",\n",
    "            \"question_type\": \"FACT\",\n",
    "            \"messages\": [],\n",
    "            \"context\": SAMPLE_CONTEXTS[\"python_info\"]\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"Invalid question type\",\n",
    "            \"question_type\": \"INVALID_TYPE\",\n",
    "            \"messages\": [HumanMessage(content=\"What is AI?\", id=\"msg1\")],\n",
    "            \"context\": SAMPLE_CONTEXTS[\"machine_learning\"]\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"None question type (should default to FACT)\",\n",
    "            \"question_type\": None,\n",
    "            \"messages\": [HumanMessage(content=\"Python이란?\", id=\"msg1\")],\n",
    "            \"context\": SAMPLE_CONTEXTS[\"python_info\"]\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"Empty context and messages\",\n",
    "            \"question_type\": \"SUMMARY\",\n",
    "            \"messages\": [],\n",
    "            \"context\": \"\"\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"Very long context\",\n",
    "            \"question_type\": \"FACT\",\n",
    "            \"messages\": [HumanMessage(content=\"이 내용에 대해 알려주세요\", id=\"msg1\")],\n",
    "            \"context\": SAMPLE_CONTEXTS[\"machine_learning\"] * 10  # Very long context\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        print(f\"\\nTest {i}: {test_case['description']}\")\n",
    "        \n",
    "        state = create_test_state(\n",
    "            question_type=test_case[\"question_type\"],\n",
    "            messages=test_case[\"messages\"],\n",
    "            context=test_case[\"context\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"  Question type: {test_case['question_type']}\")\n",
    "        print(f\"  Messages count: {len(test_case['messages'])}\")\n",
    "        print(f\"  Context length: {len(test_case['context'])} chars\")\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            result = await generate_answer(state)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            answer = result.get(\"answer\", \"\")\n",
    "            model_used = result.get(\"model_used\", \"\")\n",
    "            messages = result.get(\"messages\", [])\n",
    "            \n",
    "            print(f\"  Model: {model_used}\")\n",
    "            print(f\"  Generation time: {end_time - start_time:.2f}s\")\n",
    "            print(f\"  Answer length: {len(answer)} chars\")\n",
    "            print(f\"  Messages returned: {len(messages)}\")\n",
    "            \n",
    "            if answer == ERROR_MESSAGE:\n",
    "                print(f\"  ⚠️ Error response as expected\")\n",
    "            elif answer:\n",
    "                print(f\"  ✅ Response generated\")\n",
    "                print(f\"  Answer preview: {answer[:150]}{'...' if len(answer) > 150 else ''}\")\n",
    "            else:\n",
    "                print(f\"  ❌ Empty response\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Exception: {e}\")\n",
    "\n",
    "await test_edge_cases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case 6: Prompt Selection Logic\n",
    "\n",
    "Test that the correct prompt is selected for each question type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_prompt_selection():\n",
    "    print(\"=== Prompt Selection Logic Test ===\")\n",
    "    \n",
    "    # Test each question type to ensure correct prompt usage\n",
    "    question_types = [\"FACT\", \"SUMMARY\", \"COMPARE\", \"EVIDENCE\"]\n",
    "    \n",
    "    for question_type in question_types:\n",
    "        print(f\"\\nTesting {question_type} prompt selection:\")\n",
    "        \n",
    "        state = create_test_state(\n",
    "            question_type=question_type,\n",
    "            messages=[HumanMessage(content=f\"{question_type} 타입 질문입니다\", id=\"msg1\")],\n",
    "            context=SAMPLE_CONTEXTS[\"python_info\"]\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            result = await generate_answer(state)\n",
    "            answer = result.get(\"answer\", \"\")\n",
    "            model_used = result.get(\"model_used\", \"\")\n",
    "            \n",
    "            print(f\"  Question type: {question_type}\")\n",
    "            print(f\"  Model: {model_used}\")\n",
    "            print(f\"  Answer generated: {'Yes' if answer and answer != ERROR_MESSAGE else 'No'}\")\n",
    "            print(f\"  Answer length: {len(answer)} chars\")\n",
    "            \n",
    "            # Check if prompt was selected correctly (indirect test)\n",
    "            if question_type in PROMPT_MAP:\n",
    "                print(f\"  ✅ Valid prompt type used\")\n",
    "            else:\n",
    "                print(f\"  ⚠️ Fallback to FACT prompt expected\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error: {e}\")\n",
    "\n",
    "await test_prompt_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance and Quality Analysis\n",
    "\n",
    "Analyze generation performance and quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def performance_analysis():\n",
    "    print(\"=== Performance and Quality Analysis ===\")\n",
    "    \n",
    "    # Test multiple generations for timing analysis\n",
    "    test_state = create_test_state(\n",
    "        question_type=\"FACT\",\n",
    "        messages=[HumanMessage(content=\"Python의 특징을 설명해주세요\", id=\"msg1\")],\n",
    "        context=SAMPLE_CONTEXTS[\"python_info\"]\n",
    "    )\n",
    "    \n",
    "    print(\"Running 5 consecutive generations for performance analysis...\")\n",
    "    \n",
    "    generation_times = []\n",
    "    answer_lengths = []\n",
    "    \n",
    "    for i in range(5):\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            result = await generate_answer(test_state)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            generation_time = end_time - start_time\n",
    "            answer = result.get(\"answer\", \"\")\n",
    "            \n",
    "            generation_times.append(generation_time)\n",
    "            answer_lengths.append(len(answer))\n",
    "            \n",
    "            print(f\"  Run {i+1}: {generation_time:.2f}s, {len(answer)} chars\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Run {i+1}: Error - {e}\")\n",
    "    \n",
    "    if generation_times:\n",
    "        avg_time = sum(generation_times) / len(generation_times)\n",
    "        min_time = min(generation_times)\n",
    "        max_time = max(generation_times)\n",
    "        \n",
    "        avg_length = sum(answer_lengths) / len(answer_lengths)\n",
    "        min_length = min(answer_lengths)\n",
    "        max_length = max(answer_lengths)\n",
    "        \n",
    "        print(f\"\\nPerformance Summary:\")\n",
    "        print(f\"  Average generation time: {avg_time:.2f}s\")\n",
    "        print(f\"  Min generation time: {min_time:.2f}s\")\n",
    "        print(f\"  Max generation time: {max_time:.2f}s\")\n",
    "        print(f\"  Average answer length: {avg_length:.0f} chars\")\n",
    "        print(f\"  Min answer length: {min_length} chars\")\n",
    "        print(f\"  Max answer length: {max_length} chars\")\n",
    "        \n",
    "        # Performance assessment\n",
    "        if avg_time < 3.0:\n",
    "            print(f\"  ✅ Good performance (< 3s average)\")\n",
    "        elif avg_time < 5.0:\n",
    "            print(f\"  ⚠️ Acceptable performance (3-5s average)\")\n",
    "        else:\n",
    "            print(f\"  ❌ Slow performance (> 5s average)\")\n",
    "\n",
    "await performance_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Test: Interactive Generation\n",
    "\n",
    "Test custom questions and contexts interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_custom_generation(question, question_type=\"FACT\", context=\"\"):\n",
    "    \"\"\"\n",
    "    Test generation with custom input\n",
    "    \"\"\"\n",
    "    print(f\"Custom Generation Test:\")\n",
    "    print(f\"  Question: {question}\")\n",
    "    print(f\"  Type: {question_type}\")\n",
    "    print(f\"  Context: {'Provided' if context else 'None'}\")\n",
    "    \n",
    "    state = create_test_state(\n",
    "        question_type=question_type,\n",
    "        messages=[HumanMessage(content=question, id=\"custom\")],\n",
    "        context=context\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        result = await generate_answer(state)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        answer = result.get(\"answer\", \"\")\n",
    "        model_used = result.get(\"model_used\", \"\")\n",
    "        \n",
    "        print(f\"  Model: {model_used}\")\n",
    "        print(f\"  Generation time: {end_time - start_time:.2f}s\")\n",
    "        print(f\"  Answer:\")\n",
    "        print(f\"  {answer}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "\n",
    "# Example custom tests\n",
    "print(\"=== Custom Generation Tests ===\")\n",
    "\n",
    "# Test 1: Fact question with context\n",
    "await test_custom_generation(\n",
    "    question=\"Python이 데이터 과학에 인기 있는 이유는 무엇인가요?\",\n",
    "    question_type=\"FACT\",\n",
    "    context=SAMPLE_CONTEXTS[\"machine_learning\"]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Test 2: Summary without context\n",
    "await test_custom_generation(\n",
    "    question=\"클라우드 컴퓨팅에 대해 요약해주세요\",\n",
    "    question_type=\"SUMMARY\",\n",
    "    context=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Results Summary\n",
    "\n",
    "Overall test summary and recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATOR MODULE TEST SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nKey Test Areas Covered:\")\n",
    "print(\"  ✓ FACT question generation\")\n",
    "print(\"  ✓ SUMMARY question generation\")\n",
    "print(\"  ✓ COMPARE question generation\")\n",
    "print(\"  ✓ EVIDENCE question generation\")\n",
    "print(\"  ✓ Edge cases and error handling\")\n",
    "print(\"  ✓ Prompt selection logic\")\n",
    "print(\"  ✓ Performance analysis\")\n",
    "print(\"  ✓ Custom generation testing\")\n",
    "\n",
    "print(\"\\nPrompt Types Available:\")\n",
    "for prompt_type in PROMPT_MAP.keys():\n",
    "    print(f\"  - {prompt_type}\")\n",
    "\n",
    "print(\"\\nRecommendations for Production:\")\n",
    "print(\"  1. Monitor generation times and implement timeout handling\")\n",
    "print(\"  2. Add response quality validation\")\n",
    "print(\"  3. Implement retry logic for failed generations\")\n",
    "print(\"  4. Add context length validation and truncation\")\n",
    "print(\"  5. Consider caching for frequently asked questions\")\n",
    "print(\"  6. Add metrics collection for model performance\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
