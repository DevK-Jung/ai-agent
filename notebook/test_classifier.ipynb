{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Classifier Node Test Notebook\n",
    "\n",
    "This notebook tests the `classifier.py` module functionality with various question types."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T06:50:36.525054Z",
     "start_time": "2026-02-20T06:50:36.515568Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# .env íŒŒì¼ ë¡œë“œ\n",
    "dotenv_path = find_dotenv()\n",
    "load_dotenv(dotenv_path, override=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T06:50:40.463996Z",
     "start_time": "2026-02-20T06:50:37.801723Z"
    }
   },
   "source": [
    "# Import required modules\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from app.agents.nodes.classifier import classify_question\n",
    "from app.agents.prompts.classification import VALID_QUESTION_TYPES, DEFAULT_QUESTION_TYPE\n",
    "from app.agents.state import ChatState"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimjunghyeon/Desktop/workspace/ai-agent/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Setup\n",
    "\n",
    "Create mock chat states with different question types for testing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T06:50:57.270408Z",
     "start_time": "2026-02-20T06:50:57.264752Z"
    }
   },
   "source": [
    "# Test Case 1: FACT questions\n",
    "def create_fact_question_states():\n",
    "    return [\n",
    "        {\n",
    "            \"messages\": [HumanMessage(content=\"íŒŒì´ì¬ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?\", id=\"msg1\")],\n",
    "            \"expected\": \"FACT\",\n",
    "            \"description\": \"Basic fact question\"\n",
    "        },\n",
    "        {\n",
    "            \"messages\": [\n",
    "                AIMessage(content=\"ì•ˆë…•í•˜ì„¸ìš”!\", id=\"msg1\"),\n",
    "                HumanMessage(content=\"FastAPIì˜ ì£¼ìš” íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€ìš”?\", id=\"msg2\")\n",
    "            ],\n",
    "            \"expected\": \"FACT\",\n",
    "            \"description\": \"Technical fact question\"\n",
    "        },\n",
    "        {\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=\"Docker ì»¨í…Œì´ë„ˆë€?\", id=\"msg1\")\n",
    "            ],\n",
    "            \"expected\": \"FACT\",\n",
    "            \"description\": \"Definition question\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# Test Case 2: SUMMARY questions\n",
    "def create_summary_question_states():\n",
    "    return [\n",
    "        {\n",
    "            \"messages\": [HumanMessage(content=\"íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì— ëŒ€í•´ ìš”ì•½í•´ì£¼ì„¸ìš”\", id=\"msg1\")],\n",
    "            \"expected\": \"SUMMARY\",\n",
    "            \"description\": \"Direct summary request\"\n",
    "        },\n",
    "        {\n",
    "            \"messages\": [HumanMessage(content=\"ì´ ë¬¸ì„œì˜ ë‚´ìš©ì„ ê°„ë‹¨íˆ ì •ë¦¬í•´ì£¼ì„¸ìš”\", id=\"msg1\")],\n",
    "            \"expected\": \"SUMMARY\",\n",
    "            \"description\": \"Document summary request\"\n",
    "        },\n",
    "        {\n",
    "            \"messages\": [\n",
    "                AIMessage(content=\"ë„¤, ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\", id=\"msg1\"),\n",
    "                HumanMessage(content=\"ì§€ê¸ˆê¹Œì§€ ë…¼ì˜í•œ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”\", id=\"msg2\")\n",
    "            ],\n",
    "            \"expected\": \"SUMMARY\",\n",
    "            \"description\": \"Conversation summary request\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# Test Case 3: COMPARE questions\n",
    "def create_compare_question_states():\n",
    "    return [\n",
    "        {\n",
    "            \"messages\": [HumanMessage(content=\"Pythonê³¼ Javaì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\", id=\"msg1\")],\n",
    "            \"expected\": \"COMPARE\",\n",
    "            \"description\": \"Language comparison\"\n",
    "        },\n",
    "        {\n",
    "            \"messages\": [HumanMessage(content=\"Djangoì™€ FastAPI ì¤‘ ì–´ë–¤ ê²ƒì´ ë” ì¢‹ì€ê°€ìš”?\", id=\"msg1\")],\n",
    "            \"expected\": \"COMPARE\",\n",
    "            \"description\": \"Framework comparison with preference\"\n",
    "        },\n",
    "        {\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=\"ì•ˆë…•í•˜ì„¸ìš”\", id=\"msg1\"),\n",
    "                AIMessage(content=\"ì•ˆë…•í•˜ì„¸ìš”! ë„ì™€ë“œë¦´ê²Œìš”.\", id=\"msg2\"),\n",
    "                HumanMessage(content=\"MySQLê³¼ PostgreSQLì„ ë¹„êµí•´ì£¼ì„¸ìš”\", id=\"msg3\")\n",
    "            ],\n",
    "            \"expected\": \"COMPARE\",\n",
    "            \"description\": \"Database comparison in conversation\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# Test Case 4: EVIDENCE questions\n",
    "def create_evidence_question_states():\n",
    "    return [\n",
    "        {\n",
    "            \"messages\": [HumanMessage(content=\"ì™œ Pythonì´ ì¸ê¸° ìˆëŠ” ì–¸ì–´ì¸ì§€ ê·¼ê±°ë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”\", id=\"msg1\")],\n",
    "            \"expected\": \"EVIDENCE\",\n",
    "            \"description\": \"Evidence for popularity\"\n",
    "        },\n",
    "        {\n",
    "            \"messages\": [HumanMessage(content=\"ì´ ì£¼ì¥ì„ ë’·ë°›ì¹¨í•˜ëŠ” ì¦ê±°ê°€ ìˆë‚˜ìš”?\", id=\"msg1\")],\n",
    "            \"expected\": \"EVIDENCE\",\n",
    "            \"description\": \"Evidence request\"\n",
    "        },\n",
    "        {\n",
    "            \"messages\": [HumanMessage(content=\"ì´ëŸ° ê²°ë¡ ì„ ë‚´ë¦° ê·¼ê±°ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\", id=\"msg1\")],\n",
    "            \"expected\": \"EVIDENCE\",\n",
    "            \"description\": \"Reasoning evidence request\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# Test Case 5: Edge cases\n",
    "def create_edge_case_states():\n",
    "    return [\n",
    "        {\n",
    "            \"messages\": [],\n",
    "            \"expected\": DEFAULT_QUESTION_TYPE,\n",
    "            \"description\": \"Empty messages\"\n",
    "        },\n",
    "        {\n",
    "            \"messages\": [AIMessage(content=\"ì•ˆë…•í•˜ì„¸ìš”!\", id=\"msg1\")],\n",
    "            \"expected\": DEFAULT_QUESTION_TYPE,\n",
    "            \"description\": \"No human message\"\n",
    "        },\n",
    "        {\n",
    "            \"messages\": [HumanMessage(content=\"\", id=\"msg1\")],\n",
    "            \"expected\": DEFAULT_QUESTION_TYPE,\n",
    "            \"description\": \"Empty human message\"\n",
    "        },\n",
    "        {\n",
    "            \"messages\": [HumanMessage(content=\"ì•ˆë…•í•˜ì„¸ìš”\", id=\"msg1\")],\n",
    "            \"expected\": DEFAULT_QUESTION_TYPE,\n",
    "            \"description\": \"Ambiguous greeting\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "print(\"Test data created successfully!\")\n",
    "print(f\"Valid question types: {VALID_QUESTION_TYPES}\")\n",
    "print(f\"Default question type: {DEFAULT_QUESTION_TYPE}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data created successfully!\n",
      "Valid question types: {'FACT', 'COMPARE', 'EVIDENCE', 'SUMMARY'}\n",
      "Default question type: FACT\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T06:51:17.728985Z",
     "start_time": "2026-02-20T06:51:17.724679Z"
    }
   },
   "source": [
    "async def run_classification_test(test_cases, category_name):\n",
    "    \"\"\"\n",
    "    Run classification tests for a specific category\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== {category_name} Test Cases ===\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        print(f\"\\nTest {i}: {test_case['description']}\")\n",
    "        \n",
    "        # Show input\n",
    "        messages = test_case['messages']\n",
    "        if messages:\n",
    "            last_human = next(\n",
    "                (msg.content for msg in reversed(messages) if isinstance(msg, HumanMessage)),\n",
    "                \"No human message\"\n",
    "            )\n",
    "            print(f\"  Input: '{last_human}'\")\n",
    "        else:\n",
    "            print(f\"  Input: Empty messages\")\n",
    "        \n",
    "        # Run classification\n",
    "        try:\n",
    "            result = await classify_question(test_case)\n",
    "            predicted = result.get('question_type')\n",
    "            model_used = result.get('model_used')\n",
    "            \n",
    "            expected = test_case['expected']\n",
    "            is_correct = predicted == expected\n",
    "            \n",
    "            print(f\"  Expected: {expected}\")\n",
    "            print(f\"  Predicted: {predicted}\")\n",
    "            print(f\"  Model: {model_used}\")\n",
    "            print(f\"  âœ… Correct\" if is_correct else f\"  âŒ Incorrect\")\n",
    "            \n",
    "            results.append({\n",
    "                'test_case': test_case['description'],\n",
    "                'expected': expected,\n",
    "                'predicted': predicted,\n",
    "                'correct': is_correct\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error: {e}\")\n",
    "            results.append({\n",
    "                'test_case': test_case['description'],\n",
    "                'expected': test_case['expected'],\n",
    "                'predicted': 'ERROR',\n",
    "                'correct': False\n",
    "            })\n",
    "    \n",
    "    # Summary\n",
    "    correct_count = sum(1 for r in results if r['correct'])\n",
    "    total_count = len(results)\n",
    "    accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{category_name} Results: {correct_count}/{total_count} ({accuracy:.1%}) correct\")\n",
    "    \n",
    "    return results"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case 1: FACT Questions\n",
    "\n",
    "Test classification of factual information questions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T06:51:26.098513Z",
     "start_time": "2026-02-20T06:51:23.743812Z"
    }
   },
   "source": [
    "fact_results = await run_classification_test(create_fact_question_states(), \"FACT\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FACT Test Cases ===\n",
      "\n",
      "Test 1: Basic fact question\n",
      "  Input: 'íŒŒì´ì¬ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?'\n",
      "  Expected: FACT\n",
      "  Predicted: FACT\n",
      "  Model: gpt-4o-mini\n",
      "  âœ… Correct\n",
      "\n",
      "Test 2: Technical fact question\n",
      "  Input: 'FastAPIì˜ ì£¼ìš” íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€ìš”?'\n",
      "  Expected: FACT\n",
      "  Predicted: FACT\n",
      "  Model: gpt-4o-mini\n",
      "  âœ… Correct\n",
      "\n",
      "Test 3: Definition question\n",
      "  Input: 'Docker ì»¨í…Œì´ë„ˆë€?'\n",
      "  Expected: FACT\n",
      "  Predicted: FACT\n",
      "  Model: gpt-4o-mini\n",
      "  âœ… Correct\n",
      "\n",
      "FACT Results: 3/3 (100.0%) correct\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case 2: SUMMARY Questions\n",
    "\n",
    "Test classification of summary request questions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T06:51:35.547155Z",
     "start_time": "2026-02-20T06:51:33.434090Z"
    }
   },
   "source": [
    "summary_results = await run_classification_test(create_summary_question_states(), \"SUMMARY\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SUMMARY Test Cases ===\n",
      "\n",
      "Test 1: Direct summary request\n",
      "  Input: 'íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì— ëŒ€í•´ ìš”ì•½í•´ì£¼ì„¸ìš”'\n",
      "  Expected: SUMMARY\n",
      "  Predicted: SUMMARY\n",
      "  Model: gpt-4o-mini\n",
      "  âœ… Correct\n",
      "\n",
      "Test 2: Document summary request\n",
      "  Input: 'ì´ ë¬¸ì„œì˜ ë‚´ìš©ì„ ê°„ë‹¨íˆ ì •ë¦¬í•´ì£¼ì„¸ìš”'\n",
      "  Expected: SUMMARY\n",
      "  Predicted: SUMMARY\n",
      "  Model: gpt-4o-mini\n",
      "  âœ… Correct\n",
      "\n",
      "Test 3: Conversation summary request\n",
      "  Input: 'ì§€ê¸ˆê¹Œì§€ ë…¼ì˜í•œ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”'\n",
      "  Expected: SUMMARY\n",
      "  Predicted: SUMMARY\n",
      "  Model: gpt-4o-mini\n",
      "  âœ… Correct\n",
      "\n",
      "SUMMARY Results: 3/3 (100.0%) correct\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case 3: COMPARE Questions\n",
    "\n",
    "Test classification of comparison questions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T06:51:39.981110Z",
     "start_time": "2026-02-20T06:51:38.235517Z"
    }
   },
   "source": [
    "compare_results = await run_classification_test(create_compare_question_states(), \"COMPARE\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMPARE Test Cases ===\n",
      "\n",
      "Test 1: Language comparison\n",
      "  Input: 'Pythonê³¼ Javaì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?'\n",
      "  Expected: COMPARE\n",
      "  Predicted: COMPARE\n",
      "  Model: gpt-4o-mini\n",
      "  âœ… Correct\n",
      "\n",
      "Test 2: Framework comparison with preference\n",
      "  Input: 'Djangoì™€ FastAPI ì¤‘ ì–´ë–¤ ê²ƒì´ ë” ì¢‹ì€ê°€ìš”?'\n",
      "  Expected: COMPARE\n",
      "  Predicted: COMPARE\n",
      "  Model: gpt-4o-mini\n",
      "  âœ… Correct\n",
      "\n",
      "Test 3: Database comparison in conversation\n",
      "  Input: 'MySQLê³¼ PostgreSQLì„ ë¹„êµí•´ì£¼ì„¸ìš”'\n",
      "  Expected: COMPARE\n",
      "  Predicted: COMPARE\n",
      "  Model: gpt-4o-mini\n",
      "  âœ… Correct\n",
      "\n",
      "COMPARE Results: 3/3 (100.0%) correct\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case 4: EVIDENCE Questions\n",
    "\n",
    "Test classification of evidence/reasoning questions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T06:51:44.775336Z",
     "start_time": "2026-02-20T06:51:42.752111Z"
    }
   },
   "source": [
    "evidence_results = await run_classification_test(create_evidence_question_states(), \"EVIDENCE\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EVIDENCE Test Cases ===\n",
      "\n",
      "Test 1: Evidence for popularity\n",
      "  Input: 'ì™œ Pythonì´ ì¸ê¸° ìˆëŠ” ì–¸ì–´ì¸ì§€ ê·¼ê±°ë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”'\n",
      "  Expected: EVIDENCE\n",
      "  Predicted: EVIDENCE\n",
      "  Model: gpt-4o-mini\n",
      "  âœ… Correct\n",
      "\n",
      "Test 2: Evidence request\n",
      "  Input: 'ì´ ì£¼ì¥ì„ ë’·ë°›ì¹¨í•˜ëŠ” ì¦ê±°ê°€ ìˆë‚˜ìš”?'\n",
      "  Expected: EVIDENCE\n",
      "  Predicted: EVIDENCE\n",
      "  Model: gpt-4o-mini\n",
      "  âœ… Correct\n",
      "\n",
      "Test 3: Reasoning evidence request\n",
      "  Input: 'ì´ëŸ° ê²°ë¡ ì„ ë‚´ë¦° ê·¼ê±°ëŠ” ë¬´ì—‡ì¸ê°€ìš”?'\n",
      "  Expected: EVIDENCE\n",
      "  Predicted: EVIDENCE\n",
      "  Model: gpt-4o-mini\n",
      "  âœ… Correct\n",
      "\n",
      "EVIDENCE Results: 3/3 (100.0%) correct\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case 5: Edge Cases\n",
    "\n",
    "Test classification of edge cases and ambiguous inputs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T06:51:49.913165Z",
     "start_time": "2026-02-20T06:51:47.499368Z"
    }
   },
   "source": [
    "edge_results = await run_classification_test(create_edge_case_states(), \"EDGE CASES\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EDGE CASES Test Cases ===\n",
      "\n",
      "Test 1: Empty messages\n",
      "  Input: Empty messages\n",
      "  Expected: FACT\n",
      "  Predicted: FACT\n",
      "  Model: gpt-4o-mini\n",
      "  âœ… Correct\n",
      "\n",
      "Test 2: No human message\n",
      "  Input: 'No human message'\n",
      "  Expected: FACT\n",
      "  Predicted: FACT\n",
      "  Model: gpt-4o-mini\n",
      "  âœ… Correct\n",
      "\n",
      "Test 3: Empty human message\n",
      "  Input: ''\n",
      "  Expected: FACT\n",
      "  Predicted: FACT\n",
      "  Model: gpt-4o-mini\n",
      "  âœ… Correct\n",
      "\n",
      "Test 4: Ambiguous greeting\n",
      "  Input: 'ì•ˆë…•í•˜ì„¸ìš”'\n",
      "  Expected: FACT\n",
      "  Predicted: FACT\n",
      "  Model: gpt-4o-mini\n",
      "  âœ… Correct\n",
      "\n",
      "EDGE CASES Results: 4/4 (100.0%) correct\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Test Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T06:51:55.534921Z",
     "start_time": "2026-02-20T06:51:55.531215Z"
    }
   },
   "source": [
    "# Combine all results\n",
    "all_results = fact_results + summary_results + compare_results + evidence_results + edge_results\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"OVERALL TEST RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Overall accuracy\n",
    "total_correct = sum(1 for r in all_results if r['correct'])\n",
    "total_tests = len(all_results)\n",
    "overall_accuracy = total_correct / total_tests if total_tests > 0 else 0\n",
    "\n",
    "print(f\"\\nTotal Tests: {total_tests}\")\n",
    "print(f\"Correct: {total_correct}\")\n",
    "print(f\"Incorrect: {total_tests - total_correct}\")\n",
    "print(f\"Overall Accuracy: {overall_accuracy:.1%}\")\n",
    "\n",
    "# Accuracy by category\n",
    "print(f\"\\nAccuracy by Category:\")\n",
    "categories = [\n",
    "    (\"FACT\", fact_results),\n",
    "    (\"SUMMARY\", summary_results),\n",
    "    (\"COMPARE\", compare_results),\n",
    "    (\"EVIDENCE\", evidence_results),\n",
    "    (\"EDGE CASES\", edge_results)\n",
    "]\n",
    "\n",
    "for category_name, results in categories:\n",
    "    if results:\n",
    "        correct = sum(1 for r in results if r['correct'])\n",
    "        total = len(results)\n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        print(f\"  {category_name}: {correct}/{total} ({accuracy:.1%})\")\n",
    "\n",
    "# Show failed cases\n",
    "failed_cases = [r for r in all_results if not r['correct']]\n",
    "if failed_cases:\n",
    "    print(f\"\\nFailed Test Cases:\")\n",
    "    for case in failed_cases:\n",
    "        print(f\"  - {case['test_case']}: Expected {case['expected']}, Got {case['predicted']}\")\n",
    "else:\n",
    "    print(f\"\\nğŸ‰ All test cases passed!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "OVERALL TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "\n",
      "Total Tests: 16\n",
      "Correct: 16\n",
      "Incorrect: 0\n",
      "Overall Accuracy: 100.0%\n",
      "\n",
      "Accuracy by Category:\n",
      "  FACT: 3/3 (100.0%)\n",
      "  SUMMARY: 3/3 (100.0%)\n",
      "  COMPARE: 3/3 (100.0%)\n",
      "  EVIDENCE: 3/3 (100.0%)\n",
      "  EDGE CASES: 4/4 (100.0%)\n",
      "\n",
      "ğŸ‰ All test cases passed!\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Test: Interactive Classification\n",
    "\n",
    "Test custom questions interactively"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T06:52:01.790498Z",
     "start_time": "2026-02-20T06:51:59.862928Z"
    }
   },
   "source": [
    "async def test_custom_question(question_text):\n",
    "    \"\"\"\n",
    "    Test classification of a custom question\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        \"messages\": [HumanMessage(content=question_text, id=\"custom\")]\n",
    "    }\n",
    "    \n",
    "    print(f\"Question: '{question_text}'\")\n",
    "    \n",
    "    try:\n",
    "        result = await classify_question(state)\n",
    "        print(f\"Classification: {result.get('question_type')}\")\n",
    "        print(f\"Model: {result.get('model_used')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Test some custom questions\n",
    "custom_questions = [\n",
    "    \"ë¨¸ì‹ ëŸ¬ë‹ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "    \"ì´ ë°ì´í„°ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”\",\n",
    "    \"Aì™€ B ì¤‘ ì–´ë–¤ ê²ƒì´ ë” ë‚˜ì€ê°€ìš”?\",\n",
    "    \"ì´ ê²°ê³¼ê°€ ì •í™•í•œ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "]\n",
    "\n",
    "print(\"=== Custom Question Tests ===\")\n",
    "for i, question in enumerate(custom_questions, 1):\n",
    "    print(f\"\\nCustom Test {i}:\")\n",
    "    await test_custom_question(question)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Custom Question Tests ===\n",
      "\n",
      "Custom Test 1:\n",
      "Question: 'ë¨¸ì‹ ëŸ¬ë‹ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?'\n",
      "Classification: FACT\n",
      "Model: gpt-4o-mini\n",
      "\n",
      "Custom Test 2:\n",
      "Question: 'ì´ ë°ì´í„°ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”'\n",
      "Classification: SUMMARY\n",
      "Model: gpt-4o-mini\n",
      "\n",
      "Custom Test 3:\n",
      "Question: 'Aì™€ B ì¤‘ ì–´ë–¤ ê²ƒì´ ë” ë‚˜ì€ê°€ìš”?'\n",
      "Classification: COMPARE\n",
      "Model: gpt-4o-mini\n",
      "\n",
      "Custom Test 4:\n",
      "Question: 'ì´ ê²°ê³¼ê°€ ì •í™•í•œ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?'\n",
      "Classification: EVIDENCE\n",
      "Model: gpt-4o-mini\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
